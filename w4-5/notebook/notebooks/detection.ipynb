{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoping at this point, we are familiar with classification, object detection can be explained as a classification with localization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "![mater](assets/mcqueen_real.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Localization (and over multiple objects)\n",
    "\n",
    "![mcqueen](assets/mcqueen.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Could be used on any kind of task where finding the location of the object(s) are of any use\n",
    "- Anything related to traffic, pedestrians, types of vehicles, drivable roads, landing zones etc.\n",
    "- Anything related to locating a disease over some type of medical imaging (MRI, Ultrasound, CT ...)\n",
    "- When designing automated stores, factories etc. (Like Amazon Go cashierless stores)\n",
    "\n",
    "this could go on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yea yea yea its all good but how does it come to be and how can I learn / use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we are kind of familiar with a CNN, it acts as a feature extractor, connects to a FCN with number of classes as neurons for output and ta-dah, we have a multi class classifier. \n",
    "\n",
    "![out_neurons](assets/detection_output_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![annotated_out_neurons](assets/annotated_output_neurons.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be penalized with any loss but main logic here is that \n",
    "loss_fn = lambda x, y: (x - y) ** 2\n",
    "prediction = [1] * 8  # P, x, y, w, h, c1, c2, c3\n",
    "label = [1] * 8\n",
    "\n",
    "if prediction[0]:\n",
    "    # calculate loss for only first neuron, we want it to be 0\n",
    "    loss = loss_fn(prediction[0], label[0])\n",
    "else:\n",
    "    # calculate loss over all the other predictions as well\n",
    "    loss = sum([loss_fn(p, l) for p, l in zip(prediction, label)])  # you do not have to use one type of loss function here\n",
    "    # you can use variation of losses which may differ from a bounding box to a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how do we classify an unknown number of objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let me explain while expanding on some utility functions that make object detection the way it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding window detection\n",
    "\n",
    "Sliding window detection is like searching for your car in a crowded parking lot - \n",
    "except instead of cars, it is any kind of object in the image. This method involves repeatedly \n",
    "applying the same feature detector or \"window\" to an image at multiple locations \n",
    "and scales. As it slides around, it checks each spot to see if there is a good match.\n",
    "\n",
    "**Example:** Imagine you're looking for your snail hiding behind you in the house. A sliding window detector would move its feature template over the \n",
    "image, checking possible locations where your cat might be hiding, at different \n",
    "scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/sliding_snail.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterable\n",
    "import numpy as np\n",
    "\n",
    "def sliding_window(image: np.array, step_size: int, window_size: Tuple[int, int]) -> Iterable[Tuple[int, int, np.ndarray]]:\n",
    "    H, W = image.shape\n",
    "    # Check if the image has two channels as expected\n",
    "    if len(image.shape) != 2:\n",
    "        raise ValueError(\"Input image should be a 2D array.\")\n",
    "    \n",
    "    for y in range(0, H, step_size):\n",
    "        for x in range(0, W, step_size):\n",
    "            yield (x, y, image[y: y + window_size[1], x: x + window_size[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding Windows over Convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![swin_conv](assets/sliding_window_conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection Over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU is a measure of how well two objects that cover an area fit together, in this case the prediction and the ground truth. \n",
    "\n",
    "IoU, stated by its name as well, simply calculates the ratio of the intersection area to the union area between two \n",
    "bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/iou-formula.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/iou-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image_1 reference](https://idiotdeveloper.com/what-is-intersection-over-union-iou/)\n",
    "\n",
    "[image_2 reference](https://www.superannotate.com/blog/intersection-over-union-for-object-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred: Tuple[int, int, int, int], \n",
    "        gt: Tuple[int, int, int, int]) -> float:\n",
    "    \"\"\" in xyxy format, you can write it as xywh format if you'd like \"\"\"\n",
    "    # intersection points\n",
    "    x1 = max(pred[0], gt[0])\n",
    "    y1 = max(pred[0], gt[0])\n",
    "    x2 = max(pred[0], gt[0])\n",
    "    y2 = max(pred[0], gt[0])\n",
    "\n",
    "    # intersection\n",
    "    intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "\n",
    "    # area of boxes\n",
    "    area_pred = (pred[2] - pred[0] + 1) * (pred[3] - pred[1] + 1)\n",
    "    area_gt = (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1)\n",
    "\n",
    "    iou = intersection / float(area_pred + area_gt - intersection)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchor boxes are like the buffet of object detection - they offer multiple choices or \"anchors\" for bounding box predictions. Instead of predicting a single box, an anchor box-based detector proposes a range of possible boxes that might contain an object.\n",
    "\n",
    "**Example:** Imagine you are trying to detect all the animals in an image. An anchor box-based detector would propose multiple bounding boxes with different sizes and aspect ratios, covering possible locations and orientations of the animals. The algorithm then adjusts these anchors based on the detected objects characteristics, like size and shape, to get a more accurate detection result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![anchor](assets/anchor_box.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_boxes(scales: list, aspect_ratios: list, image_size: Tuple[int, int]):\n",
    "    anchor_boxes = []\n",
    "    for scale in scales:  # different sizes for anchor boxes\n",
    "        for ratio in aspect_ratios:\n",
    "            width = scale * np.sqrt(ratio)\n",
    "            height = scale / np.sqrt(ratio)\n",
    "            # create anchor box\n",
    "            anchor_boxes.append([width, height])\n",
    "    return anchor_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Max Suppression (NMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that grid cells are used in anchor boxes and will learn different shapes and orientations. But running the algorithm, you will see that there are many unnecessary detections (can be observed in the image below). \n",
    "\n",
    "Non-max suppression is by name, an algorithm that supresses the bounding boxes of the same grid with lower than certain threshold and iou value with respect to the other bounding boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/nms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://learnopencv.com/weighted-boxes-fusion/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_supression(boxes: list, scores: list, threshold: float = 0.5):\n",
    "    if len(boxes) == 0:\n",
    "        return []  # no prediction to supress\n",
    "    \n",
    "    # it is good to work with np arrays / easier if it is not already that way\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # sorting bbox confidence scores in descending order\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    picked = []\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        current = indices[0]\n",
    "        picked.append(current)\n",
    "\n",
    "        # compute iou for all of the rest\n",
    "        remaining = indices[1:]\n",
    "        ious = np.array([iou(boxes[current], boxes[i]) for i in remaining])\n",
    "\n",
    "        indices = remaining[ious < threshold]  # elliminate boxes that computes iou less than the threshold\n",
    "\n",
    "    return boxes[picked]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note on how YOLO calculates loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(maybe not the current ones like YOLO7-8-9-10..., can't keep track of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_loss_fn, bce, categorical_ce = None, None, None\n",
    "\n",
    "def yolo_loss(predictions, ground_truth, anchors):\n",
    "    # Split predictions into components\n",
    "    obj_preds = predictions[..., 0]   # objectness\n",
    "    box_preds = predictions[..., 1:5]  # x, y, w, h\n",
    "    class_preds = predictions[..., 6:] # class predictions\n",
    "    \n",
    "    # \"is there\" an object?\n",
    "    obj_loss = bce(obj_preds, ground_truth[..., 0])\n",
    "    \n",
    "    # \"how much\" of the object we have correctly guessed\n",
    "    iou_loss = iou_loss_fn(box_preds, ground_truth[..., 1:5])\n",
    "    \n",
    "    # did we guess \"which\" object it is\n",
    "    class_loss = categorical_ce(class_preds, ground_truth[..., 6:])\n",
    "    \n",
    "    return iou_loss + obj_loss + class_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What YOLO does other than that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image reference](assets/yolo_dls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image reference](https://www.coursera.org/learn/convolutional-neural-networks/lecture/fF3O0/yolo-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some limitations of original yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Struggles to generalize objects that does not fit the anchor boxes, different aspect ratio objects\n",
    "- Struggles to differentiate between small errors on large boxes vs same errors in smaller boxes are huge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's just infer stuff with yolo for fun, yolov5 is in torch.hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dogukanince/miniforge3/lib/python3.10/site-packages/torch/hub.py:295: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /Users/dogukanince/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.27-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (3.8.4)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: psutil in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (5.9.8)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from ultralytics) (0.13.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.10-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.3.27-py3-none-any.whl (878 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m879.0/879.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.10-py3-none-any.whl (26 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo, ultralytics-thop, ultralytics\n",
      "Successfully installed py-cpuinfo-9.0.0 ultralytics-8.3.27 ultralytics-thop-2.0.10\n",
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/dogukanince/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'requests>=2.32.2', 'tqdm>=4.66.3', 'setuptools>=70.0.0'] not found, attempting AutoUpdate...\n",
      "Collecting gitpython>=3.1.30\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting requests>=2.32.2\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m829.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting setuptools>=70.0.0\n",
      "  Downloading setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests>=2.32.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests>=2.32.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests>=2.32.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests>=2.32.2) (2024.2.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tqdm, smmap, setuptools, requests, gitdb, gitpython\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.2.2\n",
      "    Uninstalling setuptools-68.2.2:\n",
      "      Successfully uninstalled setuptools-68.2.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed gitdb-4.0.11 gitpython-3.1.43 requests-2.32.3 setuptools-75.3.0 smmap-5.0.1 tqdm-4.66.6\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 4.6s, installed 4 packages: ['gitpython>=3.1.30', 'requests>=2.32.2', 'tqdm>=4.66.3', 'setuptools>=70.0.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-11-3 Python-3.10.13 torch-2.4.1 CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1M/14.1M [00:01<00:00, 10.5MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "/Users/dogukanince/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "image 1/5: 720x1280 2 persons, 2 ties\n",
      "image 2/5: 880x1580 2 cars, 1 boat\n",
      "image 3/5: 1080x1920 1 person\n",
      "image 4/5: 3116x4816 1 sports ball, 1 fork, 1 knife, 1 apple, 1 scissors\n",
      "image 5/5: 1080x1920 1 person, 1 chair, 2 tvs, 1 refrigerator\n",
      "Speed: 1391.5ms pre-process, 61.9ms inference, 2.2ms NMS per image at shape (5, 3, 416, 640)\n",
      "Saved 5 images to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# pred stuff on yolo\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Images\n",
    "imgs = [\n",
    "    \"https://ultralytics.com/images/zidane.jpg\",\n",
    "    \n",
    "    \"https://lumiere-a.akamaihd.net/v1/images/open-uri20150608-27674-iuiafs_2fd2629d.jpeg\",\n",
    "\n",
    "    \"https://wallpapercave.com/wp/s1o8rpn.jpg\",\n",
    "\n",
    "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.pinimg.com%2Foriginals%2F36%2Fcd%2Feb%2F36cdebcd4fdd7eef3c9d0723cb0a886e.jpg\",\n",
    "\n",
    "    \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.cctvcamerapros.com%2Fv%2Fimages%2FHD-Security-Cameras%2FHD-TVI-BL2%2Finfrared-HD-TVI-camera-1080p-surveillance.jpg\",\n",
    "    ]\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "\n",
    "# Results\n",
    "results.print()\n",
    "results.save()  # or .show()\n",
    "# results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
