{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Text Summarization with T5 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook you will perform text summarizaiton task on the XSum dataset using the T5 (Text-to-Text Transfer Transformer).\n",
        "\n",
        "Text summarization is an NLP task that involves extracting the main points and important information from a long piece of text while creating a shorter version of it without losing any of the meaning. The XSum (Extreme Summarization) dataset is a dataset containing news articles from the BBC and short, concise summaries of these articles.\n",
        "\n",
        "The input to your model will be text data from the XSum dataset, and the output will be the summarized text. The goal is to fine-tune the T5 model using this dataset to produce high-quality summaries of news articles.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gz6o0q9Ow2f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![**Each NLP problem as a â€œtext-to-textâ€ problem** - input: text, output: text](https://miro.medium.com/max/4006/1*D0J1gNQf8vrrUpKeyD8wPA.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "UAmhFU1zw7gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Text Summarization with T5 Model](#scrollTo=rEJBSTyZIrIb)\n",
        "\n",
        "[What is T5?](#scrollTo=2bvevT7-w9z1)\n",
        "\n",
        ">[Different T5 models](#scrollTo=D2Qohq12w_6A)\n",
        "\n",
        ">[2 Types of Summary Generation Mechanisms](#scrollTo=PhhXP2UgxCSf)\n",
        "\n",
        ">>[1)Extractive Summarizaiton](#scrollTo=B8BbYI5YxEXy)\n",
        "\n",
        ">>[1)Abstractive Summarizaiton](#scrollTo=NIA6DgC1xIk7)\n",
        "\n",
        "[1) Import Libraries and Set up GPU](#scrollTo=DsRrkZx9xOwB)\n",
        "\n",
        "[2) Exploratory Data Analysis and Preprocessing](#scrollTo=p7Ko5WCDyD6P)\n",
        "\n",
        ">[XSum (Extreme Summarization) Dataset](#scrollTo=4tPKytq9A6Uv)\n",
        "\n",
        "[3) Initialize The Pre-trained T5 Model and Load The Tokenizer](#scrollTo=rmCGQOH-42v8)\n",
        "\n",
        "[4) Tokenize the Dataset](#scrollTo=anMrcgM8NIvG)\n",
        "\n",
        "[5) Load The Pre-Trained T5 Model](#scrollTo=hNNNbcaA8E8_)\n",
        "\n",
        "[What is ROUGE Metric ?](#scrollTo=ki_djYN7UjqT)\n",
        "\n",
        ">[Different ROUGE Metric Types](#scrollTo=q5F3zcXsVDBj)\n",
        "\n",
        "[6) Train and Evaluate Model](#scrollTo=12lNkB9VNku8)\n",
        "\n",
        "[7) Inference from Fine-Tuned Model](#scrollTo=rQDfC7HtL8nC)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "mQTqxk3gadUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is T5?"
      ],
      "metadata": {
        "id": "2bvevT7-w9z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The T5  model is a transformer based model introduced by Google Research in 2019. T5 is based on the Transformer architecture and aims to handle most natural language processing tasks in a single **text-to-text** framework. This means that the model takes a text as input and produces a text again.\n",
        "\n",
        "Here is the link for research paper : [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683v4)\n",
        "\n",
        "For more detailed information about the T5 model, see: [T5](https://huggingface.co/docs/transformers/model_doc/t5#t5)\n",
        "\n",
        "\n",
        "### Different T5 models\n",
        "\n",
        "| Model         | Encoder Layers | Decoder Layers | Hidden Size | FFN Size | Attention Heads | Parameters   |\n",
        "|---------------|----------------|----------------|-------------|----------|-----------------|--------------|\n",
        "| T5 Small      | 6              | 6              | 512         | 2048     | 8               | 60 million   |\n",
        "| T5 Base       | 12             | 12             | 768         | 3072     | 12              | 220 million  |\n",
        "| T5 Large      | 24             | 24             | 1024        | 4096     | 16              | 770 million  |\n",
        "| T5 3B         | 24             | 24             | 1024        | 4096     | 32              | 3 billion    |\n",
        "| T5 11B        | 24             | 24             | 1024        | 4096     | 64              | 11 billion   |"
      ],
      "metadata": {
        "id": "D2Qohq12w_6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Types of Summary Generation Mechanisms"
      ],
      "metadata": {
        "id": "PhhXP2UgxCSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![**nsp](https://lh4.googleusercontent.com/83a1_AZrS1EoCxczzF_x6bObhj-nX1t81SfOC-NprCQaDbHQ7xgcTcefuIlYff-i9Gajnl8eegWlfqvseE_mHP4j8mTquhmN8-HNztC7sc_gRGbTtvJSBXvactWAe85Raky0IXNF1zQycFBcJMLk8KE)"
      ],
      "metadata": {
        "id": "yhXO-awbxL9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### 1)Extractive Summarizaiton"
      ],
      "metadata": {
        "id": "B8BbYI5YxEXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractive summarization is a technique used to generate summaries by selecting key sentences, phrases, or sections **directly from the source text**. Extractive summarizaiton can be based on ML methods, statistical methods such as **TF-IDF (Term Frequency-Inverse Document Frequency)** or graph based methods such as **TextRank** and **LexRank**.\n"
      ],
      "metadata": {
        "id": "3SbEn0zTxGlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### 1)Abstractive Summarizaiton"
      ],
      "metadata": {
        "id": "NIA6DgC1xIk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstractive summarization is a technique used to generate summaries by **creating new sentences that capture the essence of the original text**. Unlike extractive summarization, which selects and compiles sentences directly from the source text, abstractive summarization involves paraphrasing and rephrasing the information to produce a more coherent and concise summary."
      ],
      "metadata": {
        "id": "IaLwe4O7xKMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![**Extractive vs Abstractive](https://devopedia.org/images/article/261/2509.1582303438.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XqOql-bnIdz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will perform **abstractive summarizaiton** on the data with the T5 model in this notebook."
      ],
      "metadata": {
        "id": "lzqvBcCbRMH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mimgSYNURK0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1)Import Libraries and Set up GPU"
      ],
      "metadata": {
        "id": "DsRrkZx9xOwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets evaluate transformers rouge-score nltk session-info -q"
      ],
      "metadata": {
        "id": "fx_eB37-FS5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from evaluate import load\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "metadata": {
        "id": "VlORxBUxw5me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if cuda is avaliable and set gpu device.\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "8ssTq9gr6BNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Exploratory Data Analysis and Preprocessing"
      ],
      "metadata": {
        "id": "p7Ko5WCDyD6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XSum (Extreme Summarization) Dataset"
      ],
      "metadata": {
        "id": "4tPKytq9A6Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created by the BBC articles, the XSum dataset is designed for the task of extreme summarization, which aims to generate very short summaries that capture the essence of the original document. Dataset contains around 226,711 news articles, each paired with a single-sentence summary.\n",
        "\n",
        "*Narayan, S., Cohen, S. B., & Lapata, M. (2018). Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. https://arxiv.org/abs/1808.08745*\n",
        "\n",
        "Dataset: https://huggingface.co/datasets/EdinburghNLP/xsum\n"
      ],
      "metadata": {
        "id": "0ewA9Io9A4-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the XSum dataset with Hugginface's load_dataset function (https://huggingface.co/docs/datasets/loading). Then examine the data set.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "dataset = load_dataset(None,\n",
        "                       trust_remote_code=None)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "ps4xVS7a6DiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a smaller training dataset with random selections from the dataset for faster training times.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "sample_ratio = None\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "small_dataset = DatasetDict({\n",
        "    'train': dataset['train'].shuffle(seed=42).select(range(int(len(dataset['train']) * sample_ratio))),\n",
        "    'validation': dataset['validation'].shuffle(seed=42).select(range(int(len(dataset['validation']) * sample_ratio))),\n",
        "    'test': dataset['test'].shuffle(seed=42).select(range(int(len(dataset['test']) * sample_ratio)))\n",
        "})\n",
        "\n",
        "small_dataset"
      ],
      "metadata": {
        "id": "c7Eq2a776Ge4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine examples from the dataset.\n",
        "\n",
        "small_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "l2VqEv9o6Idk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Initialize The Pre-trained T5 Model and Load The Tokenizer"
      ],
      "metadata": {
        "id": "rmCGQOH-42v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the T5 model and create the tokenizer object with Transformer's AutoTokenizer class.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "model_id = None\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "TUv1dzQg6KW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the tokenizer components.\n",
        "\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "CG100d2j6MUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An example output of the T5 tokenizer.\n",
        "\n",
        "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
      ],
      "metadata": {
        "id": "OlePoGJa6OeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using one of the five available (checkpoint) versions of the T5 model, the input system must have a prefix of **\"summarize:\"** attached in order for the model to work in the correct role.\n",
        "\n"
      ],
      "metadata": {
        "id": "KaBrvgrpXoaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_id in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "    prefix = None\n",
        "else:\n",
        "    prefix = \"\"\n",
        "\n",
        "# YOUR CODE STARTS HERE"
      ],
      "metadata": {
        "id": "juhVLNlu5LdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Tokenize the Dataset"
      ],
      "metadata": {
        "id": "anMrcgM8NIvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edit model inputs with `preprocess_function` to preprocess input data and target summaries for the text summarization model."
      ],
      "metadata": {
        "id": "dkFC01SnYamL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "max_input_length = None\n",
        "max_target_length = None\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs,\n",
        "                             max_length=max_input_length,\n",
        "                             truncation=None)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"],\n",
        "                       max_length=max_target_length,\n",
        "                       truncation=None)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "e9ovAuhN5LfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(small_dataset['train'][:2])"
      ],
      "metadata": {
        "id": "GqZZD5lv6Q2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data by applying the preprocess function to all data.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "tokenized_datasets = small_dataset.map(None,\n",
        "                                      batched=None)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "5TC0QZp76Smv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Load The Pre-Trained T5 Model"
      ],
      "metadata": {
        "id": "hNNNbcaA8E8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pre-trained model with '[AutoModelForSeq2SeqLM.from_pretrained](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSeq2SeqLM)'."
      ],
      "metadata": {
        "id": "9rZz_oO5SD6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "model = None\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "model.to(device) # Move the model to the GPU. Check the model architecture."
      ],
      "metadata": {
        "id": "8Ixp43As6Uw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform fine tuning using Hugginface's Trainer class. `Seq2SeqTrainer` and `Seq2SeqTrainingArguments` inherit from the Trainer and TrainingArgument classes and they're adapted for training models for sequence-to-sequence tasks such as summarization or translation.\n",
        "\n",
        "Set training arguments using  [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/v4.43.3/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments).\n"
      ],
      "metadata": {
        "id": "wH3WHQHxSmBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "batch_size = 8 # recommended 8\n",
        "model_name = model_id.split(\"/\")[-1]\n",
        "save_directory = f\"{model_name}-finetuned-xsum\"\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir = save_directory,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=2,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=2\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqgP5smt78kY",
        "outputId": "1befa383-cebc-4c20-c662-74f7ae7660b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up a special type of data collector with [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq) that will fill not only the inputs but also the labels to the maximum length in the stack."
      ],
      "metadata": {
        "id": "bdGtFDtMTgtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer = None,\n",
        "                                       model = None)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "pEPGnYTB5LjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is ROUGE Metric ?"
      ],
      "metadata": {
        "id": "ki_djYN7UjqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics are a set of metrics used to evaluate the quality of automated text summaries and machine translations. ROUGE metrics evaluate how much of the summary overlaps with the reference summary. ROUGE calculates these overlaps by comparing word n-grams, word sequences, and word pairs.\n",
        "\n",
        "[Original research paper for ROUGE metric](https://aclanthology.org/W04-1013/)\n",
        "\n"
      ],
      "metadata": {
        "id": "N2o1g3PjU5rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### Different ROUGE Metric Types\n",
        "\n",
        "- ROUGE-N: It measures the overlap between the reference summary and the automatic summary on an **n-gram** basis (strings of words of length n).\n",
        "\n",
        "    - ROUGE-1: **Unigram (1-gram)** based scoring.\n",
        "    - ROUGE-2: **Bigram (2-gram)** based scoring\n",
        "- ROUGE-L: It measures based on the **Longest Common Subsequence (LCS).**\n",
        "- ROUGE-W: It is a **weighted version of ROUGE-L**. It gives more weight to longer common subsequences.\n",
        "- ROUGE-S: It measures the **overlap of word pairs**. It does not take into account the number of words between them.\n"
      ],
      "metadata": {
        "id": "q5F3zcXsVDBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load rouge score with metric function from evaluate library.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "metric = load(None)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "metric"
      ],
      "metadata": {
        "id": "pY-jAGz_6X5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out some examples of using the ROUGE metric.\n",
        "\n"
      ],
      "metadata": {
        "id": "IgNTnWSAvM6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [\"hello there\", \"general kenobi\"]\n",
        "references = [\"hello there\", \"general kenobi\"]\n",
        "results = metric.compute(predictions=predictions,\n",
        "                         references=references)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "nG0hwkSo6aap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [\"hello there\", \"general kenobi\"]\n",
        "references = [[\"hello\", \"there\"], [\"general kenobi\", \"general yoda\"]]\n",
        "results = metric.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "Lr6pAsO06cYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute ROUGE metrics using the model's predictions and actual labels (references) with the `compute_metrics` function."
      ],
      "metadata": {
        "id": "pOkE1ljmv0mc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSMgxM2cm-73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323078ba-0f19-4bb3-9df7-0a5368160f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# The following function calculates summarization metrics (ROUGE) to evaluate the summarization performance of the model, decodes the predictions and labels, converts them into text format, separates them into sentences and calculates the metrics\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions,\n",
        "                                           skip_special_tokens=True)  # Converts the predicted tokens back to text.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) # Replaces -100 values â€‹â€‹in labels with pad_token_id (usually this is for tokens that the model does not include in learning).\n",
        "    decoded_labels = tokenizer.batch_decode(labels,\n",
        "                                            skip_special_tokens=True) # Converts actual summaries from tokens to text.\n",
        "\n",
        "\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Train and Evaluate Model"
      ],
      "metadata": {
        "id": "12lNkB9VNku8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the trainer using `Seq2SeqTrainer` from the Hugginface trainer class.\n",
        "\n",
        "See for more detailed information:[Seq2SeqTrainer](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainer)."
      ],
      "metadata": {
        "id": "dSleZ-RVrGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=None,\n",
        "    eval_dataset=None,\n",
        "    data_collator=None,\n",
        "    tokenizer=None,\n",
        "    compute_metrics=None\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "gjMt09bv8Svb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "wY7vu_CZ6fQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model you fine tuned.\n",
        "\n",
        "trainer.save_model(save_directory)"
      ],
      "metadata": {
        "id": "-Dsp-PmuwPSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Inference from Fine-Tuned Model"
      ],
      "metadata": {
        "id": "rQDfC7HtL8nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model you fine tuned and inference on the samples in the test set.\n",
        "\n",
        "ft_model = AutoModelForSeq2SeqLM.from_pretrained(save_directory)\n",
        "ft_tokenizer = AutoTokenizer.from_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "z-pJl4NUkx0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the summarize_text function, which prepares the input text and produces the summary.\n",
        "\n",
        "def summarize_text(text):\n",
        "    inputs = ft_tokenizer.encode(\n",
        "        \"summarize: \" + text,\n",
        "        return_tensors='pt',\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    summary_ids = ft_model.generate(\n",
        "        inputs,\n",
        "        max_length=50,\n",
        "        num_beams=5,\n",
        "\n",
        "    )\n",
        "    return ft_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "y4lRVosskx2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get samples from test dataset.\n",
        "\n",
        "test_text = small_dataset['test'][50]['document']\n",
        "test_text"
      ],
      "metadata": {
        "id": "YD465S59NClK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the summary for the sample text of your choice.\n",
        "\n",
        "test_summarize = summarize_text(test_text)\n",
        "test_summarize"
      ],
      "metadata": {
        "id": "e2Dpw8kq8S1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import session_info\n",
        "session_info.show()"
      ],
      "metadata": {
        "id": "qlWSLUdNOSQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}