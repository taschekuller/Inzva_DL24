{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnpq7d-poO-h"
      },
      "source": [
        "# IMDB Movie Review Sentiment Classificaiton w/BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR959HUUpV9O"
      },
      "source": [
        "In this notebook, you will perform sentiment analysis on the IMDB Movie Review Dataset using the BERT (Bidirectional Encoder Representations from Transformers) model.\n",
        "\n",
        "Sentiment analysis is a natural language processing (NLP) task that involves determining the emotion or emotional tone behind a series of words. The IMDB movie review dataset is a well-known dataset containing 50,000 movie reviews tagged as positive or negative. Your task is to train a model that can accurately classify the emotion of a particular movie review.\n",
        "\n",
        "The input to your model will be text data from IMDB movie reviews. Each review is a string of words that will be processed and tokenized in line with the BERT model. The expected output will be binary classification: each review will be labeled as positive (1) or negative (0). Leveraging the BERT model, we aim to capture the contextual meanings of words in reviews and thus improve the accuracy of our sentiment classification. This notebook will guide you through the process of data preprocessing, model training, evaluation, and making predictions about new reviews using the BERT model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![**BERT](https://cdn-images-1.medium.com/max/1500/1*g1KBCVCITjrd9IJ7AyFqdw.png)"
      ],
      "metadata": {
        "id": "DenibSAOJM4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[IMDB Movie Review Sentiment Classificaiton w/BERT](#scrollTo=Nnpq7d-poO-h)\n",
        "\n",
        "[What is BERT?](#scrollTo=_kAGrtJjoXaZ)\n",
        "\n",
        ">[Key Features of BERT](#scrollTo=WXNeaIsJ7ZzC)\n",
        "\n",
        ">>[1)Masked Language Modelling (MLM)](#scrollTo=PED6sVVtobm9)\n",
        "\n",
        ">>[2) Next Sentence Predicition (NSP)](#scrollTo=LjkfD8-iohjS)\n",
        "\n",
        ">>[Detailed Architecture of BERT Model](#scrollTo=jn8li8xE4PRI)\n",
        "\n",
        "[1)Import Libraries and Set up GPU](#scrollTo=g_J6BlEXotA0)\n",
        "\n",
        "[2) Exploratory Data Analysis and Preprocessing](#scrollTo=lxG3xDYUjlp9)\n",
        "\n",
        ">[IMDB Movie Review Dataset](#scrollTo=gvqE-YQupqjT)\n",
        "\n",
        "[3) Initialize The Pre-trained BERT Model and Load The Tokenizer](#scrollTo=UYa5zHelfyJA)\n",
        "\n",
        ">[BERT's Word Piece Tokenization](#scrollTo=BfcfpiKmOqni)\n",
        "\n",
        ">[Special Tokens](#scrollTo=_tW94n1WNxoR)\n",
        "\n",
        "[4) Tokenize the Dataset](#scrollTo=zsb_ThOpfmGI)\n",
        "\n",
        "[5) Load The Pre-Trained BERT Model](#scrollTo=u__s-wGB2SfF)\n",
        "\n",
        "[6) Train and Evaluate Model](#scrollTo=UxTaXPXGmZoL)\n",
        "\n",
        "[7) Inference From Fine-Tuned Model](#scrollTo=wesC5b6JCmjr)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "GNHmsJfVmp3x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kAGrtJjoXaZ"
      },
      "source": [
        "# What is BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDRduf_f6Fpo"
      },
      "source": [
        "BERT, which stands for Bidirectional Encoder Representations from Transformers, developed by Google and launched in 2018, is a transformer-based large language model.\n",
        "\n",
        "Here is the link to the Original article for BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXNeaIsJ7ZzC"
      },
      "source": [
        "### Key Features of BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PED6sVVtobm9"
      },
      "source": [
        "> ### 1)Masked Language Modelling (MLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbLQ_Pg58PW-"
      },
      "source": [
        "Masked Language Modeling is one of the key pre-training tasks used in BERT to enable the model to learn context-rich word representations.\n",
        "\n",
        "In BERT's MLM, before the word sequences are given to the model, **15% of the words in each sequence are masked**, that is, replaced with a **[MASK]** token. The modified sequence is passed through the **bidirectional encoder**. The model then attempts to predict the original value of the masked words based on the context provided by other unmasked words in the sequence.\n",
        "\n",
        "For detailed information about masked language modeling, [see the link](https://huggingface.co/docs/transformers/tasks/masked_language_modeling)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![**mlm](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/MLM.png)"
      ],
      "metadata": {
        "id": "8ARPNUx-JRqJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjkfD8-iohjS"
      },
      "source": [
        "> ### 2) Next Sentence Predicition (NSP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GYaxSyR946U"
      },
      "source": [
        "Next Sentence Prediction is another pre-training task used in BERT to enable the model to understand the relationship between pairs of sentences.\n",
        "\n",
        "In the NSP task, the model is fed positive and negative sentence pairs. In positive sentence pairs, the second sentence is the actual next sentence following the first sentence in the original text. In negative sentence pairs, the second sentence is a randomly selected sentence from the corpus that does not follow the first sentence.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![**nsp](https://www.researchgate.net/publication/359791880/figure/fig4/AS:11431281093138241@1667103724679/Next-sentence-prediction-in-BERT-training.png)"
      ],
      "metadata": {
        "id": "uUymxm3RJair"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Detailed Architecture of BERT Model"
      ],
      "metadata": {
        "id": "jn8li8xE4PRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT model is divided into BERT_Base and BERT_Large depending on the number of encoder layers, number of individual attention heads and latent vector size.\n",
        "\n"
      ],
      "metadata": {
        "id": "rvX_wbUU6Kv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model      | Encoder Layers (L) | Attention Heads (A) | Hidden Size (H) |\n",
        "|------------|---------------------|---------------------|------------------|\n",
        "| BERT_BASE  | 12                  | 12                  | 768              |\n",
        "| BERT_LARGE | 24                  | 16                  | 1024             |\n"
      ],
      "metadata": {
        "id": "VTXgNmk5LZmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three main inputs to the BERT model are:\n",
        "\n",
        "* **Positional Embedding:** In this embedding type, the index number of each input token is taken. In this way, it determines the position of tokens in the sentence, allowing the model to understand and learn the order of words and capture sequential dependencies.\n",
        "\n",
        "* **Segment Embedding:** This type of embedding represents the sentence number in a sentence sequence. For example, in a two-sentence input, one set of values ​​is used for the first sentence and a different set of values ​​is used for the second sentence. This helps the model distinguish which token belongs to which sentence.\n",
        "\n",
        "* **Token Embedding:** This embedding type holds the set of tokens for words given by the token generator. The tokenizer splits the text into tokens (usually words or word pieces, in the BERT model, this is word piece tokenization, you can find detailed information in the following cells), and each token is converted into an embedding vector to be processed by the model.\n"
      ],
      "metadata": {
        "id": "35WM8rk86Gu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![BERT Inputs](https://miro.medium.com/v2/resize:fit:619/1*iJqlhZz-g6ZQJ53-rE9VvA.png)\n"
      ],
      "metadata": {
        "id": "DtrrY7nENUku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "szCA7_yY8UOo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_J6BlEXotA0"
      },
      "source": [
        "## 1)Import Libraries and Set up GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers session-info -q"
      ],
      "metadata": {
        "id": "nxVxFVeDERz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J9YJR3dAm1s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertTokenizerFast, AutoTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if cuda is avaliable and set gpu device\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "HexJ7X3l47_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxG3xDYUjlp9"
      },
      "source": [
        "## 2) Exploratory Data Analysis and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvqE-YQupqjT"
      },
      "source": [
        "### IMDB Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXiA6i_SB25t"
      },
      "source": [
        "IMDB Movie Review dataset consists of 50,000 movie reviews from the IMDB and includes sentiment labels indicating whether a review is positive or negative.\n",
        "\n",
        "*Maas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011, June). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies (pp. 142-150). https://aclanthology.org/P11-1015*\n",
        "\n",
        "Dataset: https://huggingface.co/datasets/stanfordnlp/imdb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the IMDB Review dataset with Hugginface's load_dataset function (https://huggingface.co/docs/datasets/loading). Then examine the data set.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "dataset = None\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "dataset"
      ],
      "metadata": {
        "id": "c4VCcdRe4-RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IpOCP6MCwHe"
      },
      "outputs": [],
      "source": [
        "# Create a smaller training dataset with random selections from the dataset for faster training times.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "small_train_dataset = None\n",
        "small_test_dataset = None\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset"
      ],
      "metadata": {
        "id": "Sm19F4il5AvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_dataset"
      ],
      "metadata": {
        "id": "WBtDNQ875Cpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYa5zHelfyJA"
      },
      "source": [
        "## 3) Initialize The Pre-trained BERT Model and Load The Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For detailed information on different BERT models: https://github.com/google-research/bert/blob/master/README.md"
      ],
      "metadata": {
        "id": "NvXzlcQoCvQ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87DUC4qzrRRg"
      },
      "outputs": [],
      "source": [
        "# Initialize the BERT model and create the tokenizer object with Transformer's AutoTokenizer class.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "model_id = None\n",
        "tokenizer = None\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the tokenizer components\n",
        "\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "TBZItCIl5Fcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfcfpiKmOqni"
      },
      "source": [
        "### BERT's Word Piece Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fah23si7gfPr"
      },
      "source": [
        "Word Piece Tokenization is a subword tokenization algorithm that is used in natural language processing tasks, particularly in the context of machine learning models like BERT.\n",
        "\n",
        "[For more detailed information see](https://huggingface.co/learn/nlp-course/en/chapter6/6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to tokenize the random example sentences.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "test_sentence= None\n",
        "tokens = tokenizer.tokenize(test_sentence)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "RXWcw-RW5H4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the example sentence through the tokenizer.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "output=tokenizer(test_sentence,\n",
        "                 padding= None,  # Ensure that all sequences are the same length by adding padding.\n",
        "                 truncation= None, # Truncate the sequences to the maximum length if necessary.\n",
        "                 max_length= None) # Set the maximum length of the sequences.\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f'Sentence: {test_sentence}')\n",
        "print(f'Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')\n",
        "print(f'Test sentence with the special tokens: {tokenizer.decode(output[\"input_ids\"])}')\n",
        "print(f'Input IDs, Token Type IDs, Attention Mask: {output}')"
      ],
      "metadata": {
        "id": "1s3PlPnq5KZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tW94n1WNxoR"
      },
      "source": [
        "### Special Tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTch6TF8ozM0"
      },
      "source": [
        "> ##### [SEP] (Separator Token) --  It marks the end of one sequence and the beginning of another within the same input string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM_0I9-INvKe",
        "outputId": "daadec4b-88c6-477d-ca6f-a084da8477cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[SEP]', 102)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenizer.sep_token, tokenizer.sep_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66IJZ5u0ov-r"
      },
      "source": [
        "> ##### [CLS] (Classification Token) -- It marks beginning of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PoWksFfNvM7",
        "outputId": "4bc9444a-6f2d-411a-eda4-2e018112ea16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS]', 101)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tokenizer.cls_token, tokenizer.cls_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVrwvctco3M7"
      },
      "source": [
        "> ##### [PAD] (Padding Token) -- Padding is the process of filling the shorter sequences to match the longest sequence in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apcuQsYuNvPH",
        "outputId": "e7378a43-cc30-4593-c9d8-a5d5200d75ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[PAD]', 0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tokenizer.pad_token, tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding example.\n",
        "\n",
        "texts = [\"Hello world\", \"Hello world hello world hello world\"]\n",
        "\n",
        "encoded_inputs = tokenizer(texts,\n",
        "                           padding=True,\n",
        "                           return_tensors='pt')\n",
        "\n",
        "print(encoded_inputs['input_ids'])\n",
        "print(encoded_inputs['attention_mask'])\n",
        "decoded_text_1 = tokenizer.decode(encoded_inputs['input_ids'][0])\n",
        "decoded_text_2 = tokenizer.decode(encoded_inputs['input_ids'][1])\n",
        "print(decoded_text_1)\n",
        "print(decoded_text_2)"
      ],
      "metadata": {
        "id": "s1ccvLoj5hF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFbYqdOVpEdb"
      },
      "source": [
        "> ##### [UNK] (Unknown Token) -- UNK token represent words that are not found in the vocabulary of a tokenizer or language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B56dQAapBZ-",
        "outputId": "412929ec-10ec-45d2-8008-4979560e9195"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[UNK]', 100)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tokenizer.unk_token, tokenizer.unk_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Tokenize the Dataset"
      ],
      "metadata": {
        "id": "zsb_ThOpfmGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F-j6Eb8zvxn"
      },
      "source": [
        "You can utilize the `batch_encode_plus` method from the Transformers library for [batch encoding](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding). This method is particularly useful for encoding multiple texts simultaneously, optimizing both the processing time and resource utilization. It handles padding and creates attention masks automatically, ensuring that the input sequences are properly formatted for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcmNQvHuzzJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fe3763-e777-4c4f-eac3-87e6f32e1bf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    dataset['train']['text'],\n",
        "    add_special_tokens = None,\n",
        "    return_attention_mask = None,\n",
        "    pad_to_max_length = None,\n",
        "    max_length = None,\n",
        "    return_tensors = None\n",
        ")\n",
        "\n",
        "encoded_data_val = tokenizer.batch_encode_plus(\n",
        "    dataset['test']['text'],\n",
        "    add_special_tokens = None,\n",
        "    return_attention_mask = None,\n",
        "    pad_to_max_length = None,\n",
        "    max_length = None,\n",
        "    return_tensors = None\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data_train"
      ],
      "metadata": {
        "id": "9R_rSu4Q5kwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoded data for both training and validation sets need to be prepared for input into the model.  Extract `input_ids` and `attention_masks` from the encoded training and validation data. These are necessary for the model to understand which parts of the text to focus on and which parts are filler.  Convert the labels in the dataset to tensors. In this way, the data becomes suitable for processing with PyTorch during model training and evaluation."
      ],
      "metadata": {
        "id": "joYNmjZctMgq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_vAuTgO1kkk"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "input_ids_train = None\n",
        "attention_masks_train = None\n",
        "labels_train = torch.tensor(None)\n",
        "\n",
        "input_ids_val = None\n",
        "attention_masks_val = None\n",
        "labels_val = torch.tensor(None)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use PyTorch's [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class to combine `input_ids`, `attention_masks` and `labels` into a single dataset object for each cluster."
      ],
      "metadata": {
        "id": "o2neWZ08iJuN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxkDQlSR2CLh"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "dataset_train = TensorDataset(None,\n",
        "                              None,\n",
        "                              None)\n",
        "\n",
        "dataset_val = TensorDataset(None,\n",
        "                            None,\n",
        "                            None)\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u__s-wGB2SfF"
      },
      "source": [
        "## 5) Load The Pre-Trained BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pre-trained model with '[BertForSequenceClassification.from_pretrained](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertforsequenceclassification)' and set the configuration."
      ],
      "metadata": {
        "id": "JnpZs06SOzkV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRtYNbu82RRq",
        "outputId": "abc1eb58-a33b-4f2f-88fe-aff72d4a7e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    None, # Specify the model\n",
        "    num_labels = None, # For the classifier layer, consider the number of classes in the dataset.\n",
        "    output_attentions = None, # optimize the model to only return the final prediction outputs rather than intermediate states or attention weights.\n",
        "    output_hidden_states = None # optimize the model to only return the final prediction outputs rather than intermediate states or attention weights.\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "model.to(device) # Move the model to the GPU. Check the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc_Aq7JC2fLH"
      },
      "outputs": [],
      "source": [
        "# Set up data loaders for training and validation.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "batch_size = None\n",
        "\n",
        "dataloader_train = DataLoader(\n",
        "    None,\n",
        "    sampler = RandomSampler(None),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "dataloader_val = DataLoader(\n",
        "    None,\n",
        "    sampler = SequentialSampler(None), # Unlike the training loader, the validation data loader uses a sequential sampler that processes the validation data in the original order. This is typical for validation processes where the order of data does not need to be shuffled.\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the optimizer parameters for model training.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr = None,\n",
        "    eps = None\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "optimizer # Check the optimizer components."
      ],
      "metadata": {
        "id": "foL8dyIe5ooC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5kH3G8-2lQA"
      },
      "outputs": [],
      "source": [
        "# Set up the learning rate scheduler.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "epochs = None\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = None,\n",
        "    num_training_steps = None\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Train and Evaluate Model"
      ],
      "metadata": {
        "id": "UxTaXPXGmZoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the directory where the fine tuned model will be saved."
      ],
      "metadata": {
        "id": "UkM-5VO8RoLy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwh-t5Oj2pSD"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "models_out_dir = None\n",
        "os.makedirs(models_out_dir, exist_ok=True)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluate function is designed to measure the performance of the model on the validation dataset. This function collects the results the model predicted during validation, prepares it to calculate accuracy metrics, and returns the average loss value.\n",
        "\n",
        "The model is put into eval mode, so dropout and similar operations used during training are disabled. The validation dataset is processed in batches (small pieces of data) using dataloader_val."
      ],
      "metadata": {
        "id": "GK-oL8M8CLDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input_ids, attention_mask and labels values ​​from the batch.\n",
        "\n",
        "def evaluate(dataloader_val):\n",
        "    model.eval()\n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "\n",
        "    for batch in tqdm(dataloader_val, desc=\"Evaluating\", leave=False):\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "\n",
        "        # YOUR CODE STARTS HERE\n",
        "\n",
        "        inputs = {\n",
        "            'input_ids': None,\n",
        "            'attention_mask': None,\n",
        "            'labels': None\n",
        "        }\n",
        "\n",
        "        # YOUR CODE ENDS HERE\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "\n",
        "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "    return loss_val_avg, predictions, true_vals, preds_flat"
      ],
      "metadata": {
        "id": "KFk0RFtN3BGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training loop.\n",
        "\n",
        "for epoch in tqdm(range(1, epochs + 1)):\n",
        "    model.train()\n",
        "\n",
        "    loss_train_total = 0\n",
        "    progress_bar = tqdm(dataloader_train, desc=\"Epoch {:1d}\".format(epoch), leave=False, disable=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        model.zero_grad()\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        inputs = {\n",
        "            'input_ids': batch[0],\n",
        "            'attention_mask': batch[1],\n",
        "            'labels': batch[2]\n",
        "        }\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': '{:3f}'.format(loss.item() / len(batch))})\n",
        "\n",
        "\n",
        "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
        "    tqdm.write(f'\\nEpoch {epoch}')\n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "\n",
        "    loss_val_avg, predictions, true_vals, preds_flat = evaluate(dataloader_val)\n",
        "    f1 = f1_score(true_vals, preds_flat, average='weighted')\n",
        "    tqdm.write(f'Validation Loss: {loss_val_avg}')\n",
        "    tqdm.write(f'Validation F1 Score: {f1}')\n",
        "\n",
        "    torch.save(model.state_dict(), '{}/BERT_ft_epoch{}.model'.format(models_out_dir, epoch))"
      ],
      "metadata": {
        "id": "Rq-CiazlCJQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Inference From Fine-Tuned Model"
      ],
      "metadata": {
        "id": "wesC5b6JCmjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the weights of the trained model.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "model.load_state_dict(torch.load(None))\n",
        "model.to(device)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "DvttZ7uzCpzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take samples from the test dataset then tokenize and compare the ground truths with the model's predictions."
      ],
      "metadata": {
        "id": "empLLDEeCvlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "text = None\n",
        "true_label = None\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "text"
      ],
      "metadata": {
        "id": "RDq9r8WUCv0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer.encode_plus(\n",
        "    text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=256,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "input_id = encoded_input['input_ids'].to(device) # Move input ids and attention masks to the same device as the model.\n",
        "attention_mask = encoded_input['attention_mask'].to(device)\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_ids=input_id, attention_mask=attention_mask)\n",
        "    logits = output.logits\n",
        "    prediction = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"True Label: {true_label}\")\n",
        "print(f\"Prediction: {prediction}\")"
      ],
      "metadata": {
        "id": "7YlQxxl_C1pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import session_info\n",
        "#session_info.show()\n",
        "session_info.show(excludes=['pybind11_abseil'])"
      ],
      "metadata": {
        "id": "zYk1ec25C3pW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}