{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Llama3 Fine Tuning With Alpaca Dataset"
      ],
      "metadata": {
        "id": "HOS9lneSmTN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will fine-tune the LLama 3 (8B parameters) model using a customized variation of the Alpaca dataset. The Alpaca dataset, originally designed for **instruction-following** capabilities, will be modified to suit our specific objectives, enabling the model to generate more relevant and accurate outputs for the chosen task.\n",
        "\n",
        "This notebook provides a step-by-step guide to preprocessing the dataset, configuring the fine-tuning pipeline, and evaluating the model's performance post-training.\n"
      ],
      "metadata": {
        "id": "do6z-RANmY9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![**Llama3](https://hub-apac-1.lobeobjects.space/blog/assets/98885a84481f8c3a76635b750bbff33c.webp)"
      ],
      "metadata": {
        "id": "wT_XCD_la5IA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Llama3 Fine Tuning With Alpaca Dataset](#scrollTo=HOS9lneSmTN9)\n",
        "\n",
        ">[What is Llama3 ?](#scrollTo=VxGQFTt_nS86)\n",
        "\n",
        ">[Supervised Fine-Tuning for LLM Training](#scrollTo=9cQrA-G61O8j)\n",
        "\n",
        ">>[1)Full Fine-Tuning:](#scrollTo=QrB_EM1OEli3)\n",
        "\n",
        ">>[2)Low-Rank Adaptation (LoRA):](#scrollTo=324hEme2E8wb)\n",
        "\n",
        ">>[3)Quantization-aware Low-Rank Adaptation (QLoRA):](#scrollTo=t_roZH_0d55l)\n",
        "\n",
        "[1) Import Libraries and Set up GPU](#scrollTo=VjQWl_BZoOZS)\n",
        "\n",
        "[2)  Initialize The Llama 3 Model and Load The Tokenizer](#scrollTo=SJyy8v1vkvlz)\n",
        "\n",
        "[3) Exploratory Data Analysis and Preprocessing](#scrollTo=tkg_J5qSpBB4)\n",
        "\n",
        ">>[Alpaca Dataset](#scrollTo=rO1xOSAzqhx4)\n",
        "\n",
        ">>[Chat Template](#scrollTo=u73Vivw8JSvc)\n",
        "\n",
        "[4) Training Model](#scrollTo=U2Mb5opfkRuv)\n",
        "\n",
        "[5) Inference From Fıne-Tuned Model](#scrollTo=RzswT6foyB0F)\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "iZpQ1F3LmGN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Llama3 ?"
      ],
      "metadata": {
        "id": "VxGQFTt_nS86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 3 is one of the versions in the LLama language model family released by Meta. It has been published in 8B and 70B sizes as both pre-trained and instruction-tuned variants. Both versions 8 and 70B use Grouped Query Attention (GQA).\n",
        "\n",
        "\n",
        "\n",
        "Meta continued the development of the Llama language model family and released Llama 3.1 and Llama 3.2. While Llama 3.1 is available in 8B, 70B and 405B parameter sizes, it is capable of handling larger texts with context window support for up to 128,000 tokens. Llama 3.2, on the other hand, includes text-based models with 1D and 3D parameters, as well as audiovisual models with 11D and 90B parameters. This version stands out for its ability to process text and visual data together.\n",
        "\n",
        "For more detailed information: https://ai.meta.com/blog/meta-llama-3/\n",
        "\n",
        "We will use the Llama3 -8B version in this notebook, but different versions can also be used.\n"
      ],
      "metadata": {
        "id": "GBenhMy02SlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Fine-Tuning for LLM Training"
      ],
      "metadata": {
        "id": "9cQrA-G61O8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Fine-Tuning (SFT) is a method to enhance and customize pre-trained large language models (LLMs). It involves **retraining base models using a smaller dataset of instructions and responses**, transforming a basic text prediction model into one that can follow specific instructions and answer questions effectively.\n"
      ],
      "metadata": {
        "id": "ArL_hIco2P3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c51db77-8d97-45a9-bd2c-d71e930ff0b8_2292x1234.png)\n"
      ],
      "metadata": {
        "id": "3hL1PDQu1Q5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1)Full Fine-Tuning**:\n",
        "\n",
        "Full fine-tuning is a method where **all the parameters of a pre-trained model are re-trained** on a new dataset. In full fine-tuning, the entire model adapts to the new task by adjusting every parameter, allowing the model to learn the specifics of the new data comprehensively.\n"
      ],
      "metadata": {
        "id": "QrB_EM1OEli3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2)Low-Rank Adaptation (LoRA)**:\n",
        "\n",
        "Low-Rank Adaptation (LoRA) is a **parameter-efficient fine-tuning technique** used in deep learning **to adapt large pre-trained models to new tasks without retraining all the model’s parameters**. LoRA works by injecting additional, **low-rank weight matrices** into specific layers of the model, typically in attention layers or fully connected layers, while keeping the original model parameters frozen. This approach significantly reduces the number of trainable parameters and memory requirements, making it faster and more resource-efficient compared to full fine-tuning.\n"
      ],
      "metadata": {
        "id": "324hEme2E8wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Lora vs Full-ft](https://weeklyreport.ai/_astro/Diagram-2.J9V7jjP8_Z1vTWE9.webp)"
      ],
      "metadata": {
        "id": "G741BF9Of8td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Lora vs Full-ft](https://miro.medium.com/v2/resize:fit:2000/format:webp/0*D74YMwWTzyEfaRdj.png)"
      ],
      "metadata": {
        "id": "4Iu4u91kgHVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3)Quantization-aware Low-Rank Adaptation (QLoRA)**:\n",
        "\n",
        "Quantization-aware Low-Rank Adaptation (QLoRA) is an advanced technique that **combines quantization and low-rank adaptation** to make the fine-tuning of large language models (LLMs) more efficient in terms of both memory and computation.\n",
        "\n"
      ],
      "metadata": {
        "id": "t_roZH_0d55l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   QLoRA first applies quantization-aware training, typically in **4-bit or 8-bit precision**, to the model. This reduces the precision of the model's weights which significantly reduces the memory usage. By quantizing only the base model parameters (which are kept frozen during adaptation), QLoRA minimizes memory usage while preserving most of the model’s expressive capacity.\n",
        "*   On top of the quantized model, QLoRA applies **low-rank adaptation layers to specific parts of the model**. LoRA inserts small, low-rank matrices into the model that can be fine-tuned to learn task-specific information without modifying the quantized base model parameters. This allows QLoRA to retain the original model's capabilities while adapting to new data efficiently.\n",
        "\n",
        "*   Since the model is quantized, QLoRA uses quantization-aware training methods, meaning it fine-tunes with the knowledge of quantized parameters, adjusting the low-rank layers to work effectively in this lower precision environment.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hLAkyntrGdDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e7d97cd72e210afa0bd_lora-qlora.png)\n"
      ],
      "metadata": {
        "id": "vDFKy1JaHYNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FA4g5bwioMRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Import Libraries and Set up GPU"
      ],
      "metadata": {
        "id": "VjQWl_BZoOZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets -q\n",
        "! pip install git+https://github.com/huggingface/transformers -q\n",
        "! pip install trl peft accelerate bitsandbytes session-info -q\n",
        "! pip install -U wandb -q"
      ],
      "metadata": {
        "id": "kBtfOffTv0zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install -qqq flash-attn\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "    torch_dtype = torch.bfloat16\n",
        "else:\n",
        "    attn_implementation = \"eager\"\n",
        "    torch_dtype = torch.float16"
      ],
      "metadata": {
        "id": "exAzznRl7cds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, TextStreamer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "D4UOiQiQmNoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if cuda is avaliable and set gpu device.\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "4nr-3oxnv319"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2)  Initialize The Llama 3 Model and Load The Tokenizer"
      ],
      "metadata": {
        "id": "SJyy8v1vkvlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Llama-3-8B model as quantized along with the quantizaiton configs.\n",
        "\n",
        "https://huggingface.co/meta-llama/Meta-Llama-3-8B"
      ],
      "metadata": {
        "id": "9fvh8_i9HcvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Llama3-8B model.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "model_id = None\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "mo_XvSOioJJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the quantization configs. https://huggingface.co/docs/transformers/main/quantization/bitsandbytes\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=None,\n",
        "    bnb_4bit_use_double_quant=None,\n",
        "    bnb_4bit_quant_type=None,\n",
        "    bnb_4bit_compute_dtype=None\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "CDdIwEImoJMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model with hugginface's AutoModelForCausalLM class. https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    None,\n",
        "    quantization_config=None,\n",
        "    use_cache=None,\n",
        "    token = None,\n",
        "    attn_implementation=None\n",
        ")\n",
        "\n",
        "\n",
        "base_model.to(device) # Move to base model to GPU. Check the base model's components.\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "mKUmdwFO268v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the LoRA configs\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=None,\n",
        "        lora_dropout=None,\n",
        "        r=None,\n",
        "        bias=None,\n",
        "        task_type=None,\n",
        "        target_modules=None\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "G8KL-KBlktdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set PEFT in the base model and examine the ratio of the number of parameters trainable in the final state to the total number of parameters of the model.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "base_model = get_peft_model(None, None)\n",
        "base_model.print_trainable_parameters()\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "bxgHYtLi29aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the new base model's components.\n",
        "\n",
        "base_model"
      ],
      "metadata": {
        "id": "03EdugpZ2_d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the tokenizer object with Transformer's AutoTokenizer class.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(None,\n",
        "                                          token=None)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "Cnm3QfZl3BGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the special tokens of tokenizer.\n",
        "\n",
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "3quDfzi13C5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Exploratory Data Analysis and Preprocessing"
      ],
      "metadata": {
        "id": "tkg_J5qSpBB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alpaca Dataset"
      ],
      "metadata": {
        "id": "rO1xOSAzqhx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Alpaca dataset is a dataset developed by Stanford University. It consists of input and instruction-output response pairs, enabling the model to learn how to respond when given a specific command. It contains more than 52,000 examples. The data in Alpaca are in English.\n",
        "\n",
        "For more detail : https://github.com/tatsu-lab/stanford_alpaca#data-release\n",
        "\n",
        "The cleaned alpaca dataset is a cleaned variation of the alpaca dataset.\n",
        "The data fields are as follows:\n",
        "\n",
        "*   **instruction**: describes the task the model should perform. Each of the 52K instructions is unique.\n",
        "*   **input**: optional context or input for the task.\n",
        "*   **output**: the answer to the instruction as generated by text-davinci-003.\n",
        "\n",
        "Dataset: https://huggingface.co/datasets/yahma/alpaca-cleaned"
      ],
      "metadata": {
        "id": "WNXImEcaqlyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the alpaca cleaned dataset with Hugginface's load_dataset function (https://huggingface.co/docs/datasets/loading). Then examine the data set.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "dataset = load_dataset(None,\n",
        "                       split = None)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "dataset"
      ],
      "metadata": {
        "id": "FzuWzl6t3GcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a smaller training dataset with random selections from the dataset for faster training times.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "sample_ratio = None\n",
        "small_dataset = dataset.shuffle(seed=None).select(range(int(len(dataset) * sample_ratio)))\n",
        "small_dataset\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "I-U2Nup13H94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine examples from the dataset.\n",
        "\n",
        "small_dataset[0]"
      ],
      "metadata": {
        "id": "1SdU_oZg3Jrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Template"
      ],
      "metadata": {
        "id": "u73Vivw8JSvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A chat template is a structure used to organize user input and model responses of a language model in a specific format. These templates ensure that the model produces accurate and consistent responses in accordance with the data format used during training of the model.\n",
        "\n",
        "You will use the Alpaca chat template to format the data you will give as input to the model in the fine tune process. https://github.com/tatsu-lab/stanford_alpaca?tab=readme-ov-file#data-release\n",
        "\n"
      ],
      "metadata": {
        "id": "TiuAFOxmfCgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chat template correctly to format the input data. After correctly mapping the characteristics of the input data to the instruction, input and response sections, create a feature called \"text\" that combines these input values ​​appropriately.\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "EOS_TOKEN = None\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = None\n",
        "    inputs       = None\n",
        "    outputs      = None\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts,}\n",
        "pass\n",
        "\n",
        "# YOUR CODE ENDS HERE\n"
      ],
      "metadata": {
        "id": "BVP3X0T2kHTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply chat template to all data.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "dataset_prepared = small_dataset.map(None,\n",
        "                                     batched = None,)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "ZsDlm_hl3cC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out examples from the dataset.\n",
        "\n",
        "dataset_prepared['text'][0]"
      ],
      "metadata": {
        "id": "xo-N4-GA3d34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Training Model"
      ],
      "metadata": {
        "id": "U2Mb5opfkRuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output direction to save the fine tuned model.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "output_dir = None\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "WICKzwd-Jp0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training arguments with the help of sft config. https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir = None,\n",
        "    per_device_train_batch_size = None,\n",
        "    gradient_accumulation_steps = None,\n",
        "    fp16 = None,\n",
        "    learning_rate = None,\n",
        "    logging_steps = None,\n",
        "    num_train_epochs = None,\n",
        "    max_seq_length = None,\n",
        "    warmup_ratio = None,\n",
        "    save_strategy = None,\n",
        "    save_steps = None,\n",
        "    load_best_model_at_end = None,\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "bDuXsj1iotSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a trainer object with the help of SFFT trainer. https://huggingface.co/docs/trl/sft_trainer#trl.SFTTrainer\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=None,\n",
        "    train_dataset=None,\n",
        "    dataset_text_field = None,\n",
        "    peft_config=None,\n",
        "    max_seq_length=None,\n",
        "    tokenizer=None,\n",
        "    args=None,\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "7pxUpzmd3gb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training.\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "aIjUoGt13iAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "1uXTFM7fk0ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine tuned model.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "output_dir = os.path.join(None, \"final_checkpoint\")\n",
        "trainer.save_model(None)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "F9HrP9st7Aku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Inference From Fıne-Tuned Model"
      ],
      "metadata": {
        "id": "RzswT6foyB0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer. Move ft model to GPU.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "ft_model = AutoModelForCausalLM.from_pretrained(None)\n",
        "ft_model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(None)\n",
        "\n",
        "# YOUR CODE ENDS HERE"
      ],
      "metadata": {
        "id": "AToGVYPr3kv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input data for the new model with Tokenizer.\n",
        "\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            None,  # instruction\n",
        "            None,  # input\n",
        "            \"\"  # output\n",
        "        )\n",
        "    ],\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "inputs.to(ft_model.device) # Move the inputs to the same device with the model."
      ],
      "metadata": {
        "id": "5aFZ9TtQ3nMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With TextStreamer you can see the output in real time.\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "_ = ft_model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
      ],
      "metadata": {
        "id": "-bIAR6KAlc7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import session_info\n",
        "session_info.show()"
      ],
      "metadata": {
        "id": "tKHj8VLV3uqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}