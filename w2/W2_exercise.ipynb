{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html for normalization \n",
    "# Check out https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html\n",
    "transform = transforms.ToTensor() \n",
    "\n",
    "\n",
    "train_dataset = # Get Fashion MNIST train set\n",
    "test_dataset  = # Get Fashion MNIST test set\n",
    "\n",
    "# Split train dataset into training and validation sets\n",
    "train_size = # Define train size as 0.9 and 0.1 as validation size\n",
    "\n",
    "val_size = #\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Check out https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader or Check out last week's notebooks\n",
    "train_loader = # Define Dataloaders\n",
    "val_loader   = # Define Dataloaders\n",
    "test_loader  = # Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST_NN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(FashionMNIST_NN, self).__init__()\n",
    "        \n",
    "        # Define your architecture\n",
    "        # self.dropout = nn.Dropout(dropout_rate)  # You can use dropout.. It's up to you\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward Propagation operations\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model, method='he'):\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            if method == 'xavier':\n",
    "                nn.init.xavier_uniform_(layer.weight)  # Xavier Initialization\n",
    "            elif method == 'kaiming':\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')  # Kaiming (He) Initialization\n",
    "\n",
    "            ### You can implement random_normal initialization.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An implementation of EarlyStopping method\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(model.state_dict(), f'checkpoint_{val_loss}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(l1_lambda=0, l2_lambda=0.01, dropout_rate=0.5, weight_init='he'):\n",
    "    model = # Define your model. Do not forget to set input dropout rate\n",
    "    initialize_weights(model, weight_init) # Initialize you weights with the choosen method\n",
    "    \n",
    "    # Checkout https://pytorch.org/docs/stable/generated/torch.optim.SGD.html (We'll talk about optimization methods next week)\n",
    "    # Define your optimizer SGD(stoshastic gradient descent) with L2 regularization (weight decay) \n",
    "    optimizer = #\n",
    "    \n",
    "\n",
    "    # Checkout https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    # Use cross entropy loss \n",
    "    criterion = #\n",
    "\n",
    "    # Checkout EarlyStopping class and please ask if you didn't get it entirely. You can use our Discord\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    \n",
    "    num_epochs = #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            # forward prop\n",
    "            # compute loss\n",
    "            \n",
    "            # Apply L1 regularization (You can try to implement L2 :))\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "            \n",
    "            # do zerograd\n",
    "            # do backward\n",
    "            # do step\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "\n",
    "                # forward prop \n",
    "                # compute loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        \n",
    "        #Add training losses\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        #Add validation losses\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Check if we do early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model \n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Do forward prop\n",
    "            # Get predictions\n",
    "\n",
    "            #all_preds.append(preds)\n",
    "            #all_labels.append(labels)\n",
    "    \n",
    "    # Check out https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "    all_preds = torch.cat(all_preds)    # Concatenates the given sequence of seq tensors in the given dimension to compute accuracy score\n",
    "    all_labels = torch.cat(all_labels)  # Concatenates the given sequence of seq tensors in the given dimension to compute accuracy score\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "    test_acc = # calcuate accuracy #accuracy_score(all_labels.cpu(), all_preds.cpu())\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    # plot train losses\n",
    "    # plot val losses \n",
    "    # add xlabel \n",
    "    # add ylabel \n",
    "    # add legend \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the experiment\n",
    "l1_lambda = 0.01  # L1 regularization strength\n",
    "l2_lambda = 0.01  # L2 regularization strength\n",
    "dropout_rate = 0.0  # Dropout probability\n",
    "weight_init = 'xavier'  # Weight initialization method (he or xavier)\n",
    "\n",
    "model, train_losses, val_losses = train_model(l1_lambda=l1_lambda, l2_lambda=l2_lambda, dropout_rate=dropout_rate, weight_init=weight_init)\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(train_losses, val_losses)\n",
    "\n",
    "# Test the model\n",
    "test_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning \n",
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search space\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.0001, 0.01],              # Learning rates \n",
    "    'dropout_rate': [0.3, 0.5, 0.7],          # Different dropout rates \n",
    "    'l1_lambda': [0.0, 0.001, 0.01],          # Different L1 regularization strengths\n",
    "    'l2_lambda': [0.0, 0.001, 0.01],          # Different L2 regularization strengths\n",
    "    'weight_init': ['xavier', 'he']           # Different weight initialization methods\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "def grid_search():\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Get all possible combinations of hyperparameters using Cartesian product\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    \n",
    "    for i, combination in enumerate(param_combinations):\n",
    "        params = dict(zip(param_grid.keys(), combination))\n",
    "        print(f\"Testing combination {i + 1}/{len(param_combinations)}: {params}\")\n",
    "        \n",
    "        # Train the model with the current combination of hyperparameters\n",
    "        model, train_losses, val_losses = train_model(l1_lambda=params['l1_lambda'],\n",
    "                                                      l2_lambda=params['l2_lambda'],\n",
    "                                                      dropout_rate=params['dropout_rate'],\n",
    "                                                      weight_init=params['weight_init'])\n",
    "        \n",
    "        # Check the last validation loss\n",
    "        val_loss = val_losses[-1]\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "            print(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "# Run the random search for the best hyperparameters\n",
    "best_model_grid, best_params_grid = grid_search()\n",
    "print(\"Best Hyperparameters found by Grid Search:\")\n",
    "print(best_params_grid)\n",
    "\n",
    "print(\"Testing model from Grid Search:\")\n",
    "test_model(best_model_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "implement random search and find best params :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
