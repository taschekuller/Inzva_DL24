{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba00fe69-c542-4d6f-b7e0-e4aede40633a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4c4ee7-7aab-44a1-bdb9-97fa3fd89df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import count\n",
    "from typing import List, Dict, Any, Tuple, Union, Optional, Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cc1aa-f7e7-44c2-84a6-f72b3fdb4320",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9220a6-19c4-442d-bb57-26c6b484533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fopen(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2403e-6869-4f9f-b895-0600ea4991a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    out = data.lower()\n",
    "    out = out.replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"\\t\", \" \") # remove newlines, tabs, and carriage (/n) returns\n",
    "    #out = re.sub(r\"([.,!?;:'\\\"\\(\\)\\[\\]\\{\\}])\", r\" \\1 \", out)\n",
    "    out = re.sub(r\"\\s{2,}\", \" \", out) # remove multiple spaces\n",
    "    out = out.replace(\"_\", \"\") # remove underscores\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a247e2-cb9c-4e63-bfcb-0833a282d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dune = fopen(\"dune.txt\")\n",
    "corpus_cleaned = preprocess(dune)\n",
    "corpus_cleaned = sent_tokenize(corpus_cleaned) # split into sentences\n",
    "\n",
    "train_samples, dev_samples = train_test_split(\n",
    "    corpus_cleaned, \n",
    "    train_size=0.8, \n",
    "    test_size=0.2, \n",
    "    random_state = 137,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af157ac0-6203-4d29-96aa-5a1c2c8b36b0",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4249c8-dda6-42fe-abdb-313bb7bc9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        self.start_token = \"[START]\"\n",
    "        self.end_token = \"[END]\"\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        \n",
    "        self.vocab = defaultdict(count().__next__)\n",
    "        self.freq = defaultdict(int)\n",
    "\n",
    "        self.__init_special_tokens__()\n",
    "\n",
    "    def __init_special_tokens__(self):\n",
    "        self.vocab[self.start_token]\n",
    "        self.vocab[self.end_token]\n",
    "        self.vocab[self.pad_token]\n",
    "        self.vocab[self.unk_token]\n",
    "\n",
    "    def insert_token(self, token):\n",
    "        if token not in self.vocab:\n",
    "            self.vocab[token]\n",
    "\n",
    "    def train(self, samples: List[str]):\n",
    "\n",
    "        for sample in tqdm(samples): # for each sentence\n",
    "            for char_token in sample:  # for each character in the sentence\n",
    "                self.insert_token(char_token) # add the character to the vocabulary\n",
    "            \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.i2c = {v: k for k, v in self.vocab.items()} # invert the vocabulary to get the index to character mapping\n",
    "\n",
    "    def encode(\n",
    "        self, \n",
    "        input_text: Union[str, List],\n",
    "        max_length: Optional[int] = None,\n",
    "        preprocessing_function: Callable = lambda x: x,\n",
    "        exclude_end_token: bool = False\n",
    "    ) -> Union[List[int], List[List[int]]]:\n",
    "        input_ids = []\n",
    "        \n",
    "        if type(input_text) == str:\n",
    "            input_text = preprocessing_function(input_text)\n",
    "            input_ids.append(self.vocab.get(self.start_token))\n",
    "            \n",
    "            for char_token in input_text:\n",
    "                input_ids.append(self.vocab.get(char_token, self.vocab.get(self.unk_token))) # get the index of the character in the vocabulary\n",
    "                if max_length is not None and max_length - 1 == len(input_ids):\n",
    "                    break\n",
    "\n",
    "            if not exclude_end_token:\n",
    "                input_ids.append(self.vocab.get(self.end_token)) # add the end token to the input\n",
    "\n",
    "            if max_length is not None and len(input_ids) < max_length:\n",
    "                input_ids.extend(\n",
    "                    [self.vocab.get(self.pad_token) for _ in range(len(input_ids), max_length)] # pad the input with the pad token\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            input_text = list(map(lambda x: preprocessing_function(x), input_text)) # preprocess each input text\n",
    "            for each_input_text in input_text:\n",
    "                each_input_ids = []\n",
    "                for char_token in each_input_text:\n",
    "                    each_input_ids.append(self.vocab.get(char_token, self.vocab.get(self.unk_token))) \n",
    "                    if max_length is not None and max_length - 1 == len(each_input_ids): #\n",
    "                        break\n",
    "\n",
    "                    if not exclude_end_token:\n",
    "                        each_input_ids.append(self.vocab.get(self.end_token))\n",
    "                        \n",
    "                    if max_length is not None and len(each_input_ids) < max_length: \n",
    "                        each_input_ids.extend( \n",
    "                            [self.vocab.get(self.pad_token) for _ in range(len(each_input_ids), max_length)] # pad the input with the pad token\n",
    "                        )\n",
    "                input_ids.append(each_input_ids)\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "    def decode(\n",
    "        self, input_ids: Union[List[int], List[List[int]]]\n",
    "    ) -> Union[str, List[str]]:\n",
    "        decoded_string = []\n",
    "        \n",
    "        if type(input_ids) == list and type(input_ids[0]) == int:\n",
    "            for id in input_ids:\n",
    "                decoded_string.append(self.i2c.get(id))\n",
    "\n",
    "        else:\n",
    "            for each_input_id in input_ids:\n",
    "                decoded_string_each = []\n",
    "                for id in each_input_id:\n",
    "                    decoded_string_each.append(self.i2c.get(id))\n",
    "                decoded_string_each.append(decoded_string_each)\n",
    "\n",
    "        return decoded_string\n",
    "\n",
    "    def save(self, output_file=\"model/char.tokenizer\"):\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod # static method to load the tokenizer || @they belong to the class namespace and can be called directly on the class without needing an instance.\n",
    "    def load(output_file=\"model/char.tokenizer\"): \n",
    "        with open(output_file, \"rb\") as f: \n",
    "            tokenizer = pickle.load(f) \n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff53e728-706f-4773-abdc-99ebbf228ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf26c44acdad4e23a1e5ba04442b688e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = CharTokenizer()\n",
    "tokenizer.train(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e97dde8-cf2e-49ac-9ecf-817792acd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4bbb8-5c1a-4dc2-8b31-a98c6312dc57",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d162d1d-f9b6-480a-a1e3-a43afdfcf0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceSampler(Dataset):\n",
    "    def __init__(\n",
    "        self, tokenizer: CharTokenizer, samples: List[str], max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = samples\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        sentence = self.samples[idx]\n",
    "        input_ids = self.tokenizer.encode(sentence, self.max_length)\n",
    "        return {\n",
    "            \"input_ids\": torch.LongTensor(input_ids),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4c4ecb-30e1-48ac-a3b7-cbc1ca28084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentenceSampler(tokenizer, train_samples, max_length=64)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "dev_dataset = SentenceSampler(tokenizer, dev_samples, max_length=64)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b989f91-d8c0-4e6b-8b81-06c160e5e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    for batch in train_dataloader:\n",
    "        print(tokenizer.decode(batch[\"input_ids\"][0].tolist()))\n",
    "        break\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c36ee-2db5-426b-b7b6-da93374f98d9",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c34a1-3687-41f4-9cf0-d3d4310ab66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size: int, num_layers, embedding_size: int, \n",
    "        embedding_dropout_rate: float, hidden_size: int\n",
    "    ) -> None:\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embedding_dropout = nn.Dropout(embedding_dropout_rate)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=False,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: [B, max_seq_len]\n",
    "        out = self.embedding_dropout(self.embedding(token_ids))\n",
    "        # out: [B, max_seq_len, embed_dim]\n",
    "        out, (hidden, cell) = self.lstm(out)\n",
    "        # out: [B, max_seq_len, hidden_size]\n",
    "        hidden, cell = None, None\n",
    "        out = self.gelu(out)\n",
    "        return self.lm_head(out)\n",
    "        # return: [B, max_seq_len, vocab_size]\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        torch.nn.init.normal_(self.embedding.weight, mean=0.0, std=0.1)\n",
    "        \n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif \"weight_hh\" in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif \"bias\" in name:\n",
    "                param.data.fill_(0)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.lm_head.weight)\n",
    "        if self.lm_head.bias is not None:\n",
    "            self.lm_head.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cb865-004b-42e7-be21-81866d0e3454",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac7d53-cc00-494d-8995-0cd0725341fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.vocab.get(tokenizer.pad_token)\n",
    "end_token_id = tokenizer.vocab.get(tokenizer.end_token)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d19a06-8f57-40e7-9c8d-68527838c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerWrapper:\n",
    "    def __init__(\n",
    "        self, model: RNNLanguageModel, device: torch.device, \n",
    "        norm_threshold: Optional[float] = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size = 10, gamma = 0.1)\n",
    "        self.norm_threshold = norm_threshold\n",
    "\n",
    "    def train(self, train_dataloader, dev_dataloader, epochs, tokenizer, criterion):\n",
    "        total = len(train_dataloader) * epochs\n",
    "        \n",
    "        with tqdm(total=total, desc=\"Training Round\") as training:\n",
    "            for epoch in range(epochs):\n",
    "                train_perplexity = 0 # initialize the training perplexity ||  It measures how well a probabilistic model predicts a sample of text.\n",
    "                train_loss = 0\n",
    "                batch_count = 0\n",
    "                for step, batch in enumerate(train_dataloader):\n",
    "                    self.model.train()\n",
    "                    \n",
    "                    input_ids = batch[\"input_ids\"].to(self.device)\n",
    "\n",
    "                    out = self.model(input_ids[:, :-1])\n",
    "                    loss = criterion(out.permute(0, 2, 1), input_ids[:, 1:])\n",
    "\n",
    "                    #print(tokenizer.decode(token_ids[0, :].tolist()))\n",
    "                    #print(tokenizer.decode(targets[0, :].tolist()))\n",
    "\n",
    "                    loss.backward()\n",
    "                    clip_grad_norm(model.parameters(), 0.5)\n",
    "                    \n",
    "\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    batch_count +=1\n",
    "                \n",
    "                    training.update()\n",
    "                \n",
    "                dev_loss, dev_perplexity = self.evaluate(\n",
    "                    dev_dataloader=dev_dataloader,\n",
    "                    tokenizer=tokenizer,\n",
    "                    criterion=criterion\n",
    "                )\n",
    "\n",
    "                print(35*\"*\")\n",
    "                print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                print(f\"  - Train Loss: {train_loss/batch_count}\")\n",
    "                print(f\"  - Train Perplexity: {torch.exp(torch.tensor(train_loss/batch_count))}\")\n",
    "                print(f\"  - Eval Loss: {dev_loss}\")\n",
    "                print(f\"  - Eval Perplexity: {dev_perplexity}\")\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dev_dataloader, tokenizer, criterion):\n",
    "        self.model = self.model.to(self.device)\n",
    "        total = len(dev_dataloader)\n",
    "\n",
    "        dev_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        with tqdm(total=total, desc=\"Evaluation Round\") as evaluation:\n",
    "            for step, batch in enumerate(dev_dataloader):\n",
    "                self.model.eval()\n",
    "                \n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "\n",
    "                out = self.model(input_ids[:, :-1])\n",
    "                loss = criterion(out.permute(0, 2, 1), input_ids[:, 1:])\n",
    "\n",
    "                dev_loss += loss.item()\n",
    "                batch_count +=1\n",
    "                evaluation.update()\n",
    "\n",
    "        return dev_loss/batch_count, torch.exp(torch.tensor(dev_loss/batch_count))\n",
    "\n",
    "    @torch.inference_mode() #Gradient computation is disabled to save memory etc.\n",
    "    def generate(self, tokenizer, end_token_id, max_generation, condition: Optional[str] = None):\n",
    "        input_ids = tokenizer.encode(\n",
    "            input_text=condition if condition is not None else tokenizer.start_token,\n",
    "            max_length=None,\n",
    "            exclude_end_token=True\n",
    "        )\n",
    "\n",
    "        while max_generation:\n",
    "            input_ids_tensor = torch.LongTensor(input_ids)[None, :].to(self.device)\n",
    "            out = self.model(input_ids_tensor)\n",
    "            out_last = out[:, -1, :]\n",
    "            out_normalized = out_last.softmax(dim=-1)\n",
    "            o_pred = out_normalized.argmax(dim=-1).flatten().item()\n",
    "            input_ids.append(o_pred)\n",
    "\n",
    "            if o_pred == end_token_id:\n",
    "                break\n",
    "\n",
    "            max_generation = max_generation - 1\n",
    "\n",
    "        return tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c7351d6-8bb0-4ca2-bc4a-2e1cdc3012ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLanguageModel(tokenizer.vocab_size, 2, 256, 0.1, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e89e680-0c18-41b7-86b9-efb81485fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerWrapper(model, torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "684eb806-0145-4728-a115-60a18054436d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a6cca7a89a451f9cea25116b8b752c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Round:   0%|          | 0/6800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/qwx_ck9x7_d4myl47cwxy3lh0000gp/T/ipykernel_17811/3069643934.py:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  clip_grad_norm(model.parameters(), 0.5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6c239ea19644aaa7355e2c8a80a7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 1/20\n",
      "  - Train Loss: 2.1828316043404974\n",
      "  - Train Perplexity: 8.871390342712402\n",
      "  - Eval Loss: 1.6189646482467652\n",
      "  - Eval Perplexity: 5.047861576080322\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894da1022e4c428eb6a8dbb8cf7f7c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 2/20\n",
      "  - Train Loss: 1.446007208964404\n",
      "  - Train Perplexity: 4.246127128601074\n",
      "  - Eval Loss: 1.3855497198946336\n",
      "  - Eval Perplexity: 3.9970223903656006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5760750e684f4f78bfe849e39595f078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 3/20\n",
      "  - Train Loss: 1.2940423043335185\n",
      "  - Train Perplexity: 3.647501230239868\n",
      "  - Eval Loss: 1.3242703507928286\n",
      "  - Eval Perplexity: 3.759441375732422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378ef2b42b9540cb8ed2aecde9fa424d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 4/20\n",
      "  - Train Loss: 1.2235511061023263\n",
      "  - Train Perplexity: 3.399237632751465\n",
      "  - Eval Loss: 1.292295601087458\n",
      "  - Eval Perplexity: 3.6411354541778564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32ec7bcdc4244d7823ecfb89e2a3512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 5/20\n",
      "  - Train Loss: 1.1847949185792137\n",
      "  - Train Perplexity: 3.2700161933898926\n",
      "  - Eval Loss: 1.3018337151583503\n",
      "  - Eval Perplexity: 3.6760313510894775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17685a1e75e54861b0faf0c815398bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 6/20\n",
      "  - Train Loss: 1.164923092547585\n",
      "  - Train Perplexity: 3.205676317214966\n",
      "  - Eval Loss: 1.293555688156801\n",
      "  - Eval Perplexity: 3.6457266807556152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b28bb2b89c475eb0a7c8594acd71da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 7/20\n",
      "  - Train Loss: 1.1582732933409075\n",
      "  - Train Perplexity: 3.1844301223754883\n",
      "  - Eval Loss: 1.3016847687609054\n",
      "  - Eval Perplexity: 3.6754837036132812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf02eb46ff6e4ccfa2d0f71503836698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 8/20\n",
      "  - Train Loss: 1.1597516112467823\n",
      "  - Train Perplexity: 3.189141273498535\n",
      "  - Eval Loss: 1.2971061264767367\n",
      "  - Eval Perplexity: 3.658693552017212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2971a63ac1da4d9e828e7f1b77a915b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 9/20\n",
      "  - Train Loss: 1.1359906494617462\n",
      "  - Train Perplexity: 3.1142570972442627\n",
      "  - Eval Loss: 1.282497244722703\n",
      "  - Eval Perplexity: 3.605632781982422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171e2da2e5c14a4cbd0c128609143d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 10/20\n",
      "  - Train Loss: 1.0973355521174037\n",
      "  - Train Perplexity: 2.9961721897125244\n",
      "  - Eval Loss: 1.2774177572306464\n",
      "  - Eval Perplexity: 3.587364435195923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37f2fb42236471392ecf31d08ce33d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 11/20\n",
      "  - Train Loss: 1.0174635720603606\n",
      "  - Train Perplexity: 2.766169548034668\n",
      "  - Eval Loss: 1.2609886548098397\n",
      "  - Eval Perplexity: 3.5289087295532227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ac48798003400a9b7954d9ee9ec859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 12/20\n",
      "  - Train Loss: 0.9720761420095668\n",
      "  - Train Perplexity: 2.6434268951416016\n",
      "  - Eval Loss: 1.2578525424003602\n",
      "  - Eval Perplexity: 3.5178589820861816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c28c9b2bf004e2e9587e690af593660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 13/20\n",
      "  - Train Loss: 0.9381887288654551\n",
      "  - Train Perplexity: 2.5553488731384277\n",
      "  - Eval Loss: 1.259469797330744\n",
      "  - Eval Perplexity: 3.523552656173706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af89355e1b84a8bbd2725b28cc1632b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 14/20\n",
      "  - Train Loss: 0.9080400414326611\n",
      "  - Train Perplexity: 2.4794580936431885\n",
      "  - Eval Loss: 1.2619871910880593\n",
      "  - Eval Perplexity: 3.5324342250823975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f748bac2ed1d4c7c9ec1d1ef6734f23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 15/20\n",
      "  - Train Loss: 0.8800503914847093\n",
      "  - Train Perplexity: 2.4110212326049805\n",
      "  - Eval Loss: 1.2664564700687633\n",
      "  - Eval Perplexity: 3.5482568740844727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271b5b1e2c5248408453fbac2ed1f7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 16/20\n",
      "  - Train Loss: 0.8537456950720619\n",
      "  - Train Perplexity: 2.3484268188476562\n",
      "  - Eval Loss: 1.2728309498113743\n",
      "  - Eval Perplexity: 3.5709474086761475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dc1be66a234feab674cdae29e27a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation Round:   0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Epoch 17/20\n",
      "  - Train Loss: 0.8279372599195032\n",
      "  - Train Perplexity: 2.288593053817749\n",
      "  - Eval Loss: 1.2799165522350984\n",
      "  - Eval Perplexity: 3.596339464187622\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 38\u001b[0m, in \u001b[0;36mTrainerWrapper.train\u001b[0;34m(self, train_dataloader, dev_dataloader, epochs, tokenizer, criterion)\u001b[0m\n\u001b[1;32m     34\u001b[0m clip_grad_norm(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 38\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m batch_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m training\u001b[38;5;241m.\u001b[39mupdate()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataloader, dev_dataloader, 20, tokenizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "762b251d-adda-4cce-8647-696300f73d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.model.state_dict(), \"model/model_charlm.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf3535-90e3-4ea3-a2d9-5df3b9d2f164",
   "metadata": {},
   "source": [
    "## Load Model and Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442adeed-20f0-45e2-ab03-71671a27837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(trainer, tokenizer, end_token_id, gen_len, condition):\n",
    "    return \"\".join(trainer.generate(tokenizer, end_token_id, 100, condition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b452d85-f3fc-451e-bfdb-e822bc3864c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.load()\n",
    "pad_token_id = tokenizer.vocab.get(tokenizer.pad_token)\n",
    "end_token_id = tokenizer.vocab.get(tokenizer.end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5316474a-cd2a-42ce-9e22-e09ad27e15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLanguageModel(tokenizer.vocab_size, 2, 256, 0.1, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48e0893-cbb3-412f-b3a2-d6200870588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safak.bilici/miniconda3/envs/default/lib/python3.11/site-packages/torch/_utils.py:832: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model/model_charlm.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f2eb7d-a2e8-4495-8004-f1272168b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerWrapper(model, torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc546ab5-09b4-4116-b21e-9d6542d14cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START]house the spice wealth of the sand around them.[END]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join(trainer, tokenizer, end_token_id, 100, \"house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8025d3-f1f3-4216-9eb8-1f17664153c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
